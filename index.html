<html><head>
    <title>Abhishek's Portfolio</title>
    <link rel="icon" href="images/portfolio.png" type="image/png">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <link rel="stylesheet" href="main.css">
    <meta http-equiv="cache-control" content="no-cache">
    <meta http-equiv="expires" content="0"> 
    <meta http-equiv="pragma" content="no-cache">
</head>
<body class="">

    <!-- Wrapper -->
        <div id="wrapper">

            <!-- Intro -->
                <section class="intro">
                    <header>
                        <h1>Abhishek Rajesh Saste</h1>
                        <h3>I am an MSc graduate and aspiring AI/ML engineer who is passionate about building innovative solutions that can make a real impact in the world. 
                            
                            As an aspiring AI/ML engineer, I am always looking for new challenges and opportunities to expand my knowledge and skills. I am particularly interested in working on real-life projects that have the potential to make a positive impact on society. Whether it's developing cutting-edge algorithms for healthcare or building intelligent systems for environmental monitoring, I am committed to using my skills to create solutions that can make a real difference.</h3>
                        <ul class="actions">
                            <li><a href="Abhishek Rajesh Saste.pdf" download="Abhishek Rajesh Saste.pdf" class="button primary large"><span class="label">Download CV</span></a></li>
                        </ul>
                    </header>
                    <div class="content">
                        <span class="image fill" data-position="center"><img src="IMG_20230507_181644.jpg" alt=""></span>
                    </div>
                </section>


                 <!-- Section -->
            <section>
                <header>
                    <h2>Skills</h2>
                </header>
                <div class="content">
                    <p> </p>
                    <ul class="feature-icons">
                        <li class="icon solid fa-calculator"><strong>Mathematics:</strong> A good understanding of Linear Algebra, Calculus, Probability, and Statistics and
                            the foundational mathematical concepts that are used in Machine learning and Deep learning
                            algorithms.</li>
                        <li class="icon solid fa-code"><strong>Programming Languages:</strong> Python, C++, MATLAB</li>
                        <li class="icon solid fa-robot"><strong>AI/ML Frameworks:</strong> Keras, TensorFlow, PyTorch, Pandas, Numpy, Scikit-
                            learn, OpenCV.</li>
                        <li class="icon solid fa-robot"><strong>Softwares:</strong> Jupyter Notebook, GitHub, Docker, PyCharm, Apache Spark, Tableau.</li>
                        <li class="icon solid fa-code"><strong>Cloud Platform:</strong> AWS EC2 instance, AWS SageMaker, Microsoft Azure
                            Machine Learning, Google Cloud AI Platform.</li>
                        <li class="icon solid fa-database"><strong>Machine Learning Algorithms:</strong> Linear Regression, Logistic Regression,
                            Decision
                            Trees, Random Forest, Support Vector Machines, Naive Bayes, K-Nearest Neighbors, Neural
                            Networks,
                            Gradient Boosting, Clustering, AutoML, Transfer Learning.</li>
                        <li class="icon solid fa-image"><strong>Deep Learning Algorithms:</strong> Convolutional Neural Networks (CNNs),
                            Recurrent
                            Neural
                            Networks (RNNs), Reinforcement Learning.</li>
                        <li class="icon solid fa-area-chart"><strong>Computer Vision Algorithms:</strong>Object Detection, Image Segmentation, Face
                            Recognition,
                            Image Classification, Object Tracking.</li>
                        <li class="icon solid fa-area-chart"><strong>Natural Language Processing Algorithms:</strong>Text Classification, Sentiment
                            Analysis.</li>
                    </ul>
                    <p> </p>
                </div>
            </section>

 <section id="first">
                <header>
                    <h2>Experience</h2>

                    <h3>iNeuron Pvt Limited <br>October 2022 - April 2023  </h3>          
                   
                </header>
                <div class="content">
                    <p><strong>Machine-learning Intern </strong><br><br>
                        Project Approach: Using the available factors present in the dataset and performing classical machine learning tasks and developing end-to-end projects that involve the following tasks;
<br>•	Utilizing the available factors present in the dataset and performing classical machine learning tasks like data exploration, visualization, and manipulation.
<br>•	Preprocessing and engineering features to create datasets for machine learning applications.
<br>•	Employing Scikit-learn or Keras for model building and improving accuracies.
<br>•	Utilizing Grid search cross-validation and assisting in evaluating AI models performance and suggesting improvements and optimizations.
<br>•	Creating a UI using Python FLASK and deploying it on AWS, Azure, or Google Cloud platforms.
<br>•	Contributing to developing AI-powered applications and tools by writing clean and efficient code.

            </section>


           <section id="second">
                <header>
                   

                    <h3>Rolladome Skates <br>June 2023 - Present  </h3>          
                   
                </header>
                <div class="content">
                    <p><strong>AI Researcher (Team Leader) </strong><br><br>
                        <br>    •	Led a team of AI researchers in developing innovative solutions for the roller skating industry at Rolladome Skates.
                        <br>  •	Conducted in-depth research and analysis to identify opportunities for implementing artificial intelligence algorithms and models in various aspects of roller skating, such as performance analysis and personalized training recommendations.
                        <br>  •	Oversaw the design and implementation of AI systems, ensuring adherence to project timelines and quality standards.
                        <br>  •	Collaborated closely with cross-functional teams, including skating experts, data scientists, and software developers, to foster a collaborative environment and leverage diverse expertise.
                        <br>  •	Provided mentorship and guidance to team members, facilitating their professional growth and fostering a cohesive and high-performing team dynamic.
                        <br>  •	Played a key role in driving innovation and pushing the boundaries of AI applications in the roller skating industry.
                        <br>   •	Presented research findings, project progress, and recommendations to stakeholders, including senior management and executive teams.
                        <br>  •	Actively contributed to the strategic planning and decision-making process, aligning AI research efforts with company objectives and long-term vision.
                        <br>  •	Proactively identified opportunities for continuous improvement and optimization in AI research processes, tools, and methodologies.
                        <br>  •	Maintained up-to-date knowledge of industry trends, emerging technologies, and best practices in AI research, incorporating relevant insights into project strategies and execution.
                        

            </section>

            <section id="third">
                <header>
                    <h2>Virtual Experience and Certifications</h2>

                    <h3>Cognizant Virtual Artificial Intelligence Experience Program – May 2023:</h3>          
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://forage-uploads-prod.s3.amazonaws.com/completion-certificates/Cognizant/5N2ygyhzMWjKQmgCK_Cognizant_FH77MGBRcCReLyPrn_1685266794198_completion_certificate.pdf" class="button primary large">Certificate link</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-06-10 180609.jpg" alt=""></span>
                </header>
                <div class="content">
                        During my participation in the Cognizant Virtual Experience Program in partnership with Forage in May 2023, I had the opportunity to work on diverse and challenging projects, utilizing my skills in data analysis and machine learning. Here are the key details of my project work:

                        <br>• In order to derive valuable insights, I analyzed complex datasets using Python. Through data cleaning, preprocessing, and exploratory data analysis techniques, I uncovered patterns, trends, and correlations within the data. This enabled me to extract valuable insights that formed the foundation for actionable recommendations.

                        <br>• Employing machine learning techniques, I developed models for predictive analytics. By applying algorithms such as regression, classification, and clustering, I achieved accurate and reliable results. These models enabled me to forecast future trends, make data-driven decisions, and provide strategic recommendations.

                        <br>• Effective communication of machine learning findings was a priority in my project work. I created comprehensive reports and delivered engaging presentations to effectively convey the insights to stakeholders. By utilizing visualizations, clear explanations, and impactful storytelling, I ensured that the findings were easily understandable and actionable.

                        <br>• Implementing production-ready machine learning algorithms was a crucial aspect of my work. I optimized the algorithms for scalability and efficiency to ensure their practical application in real-world scenarios. This involved considering factors such as memory usage, computational speed, and resource requirements.

                        <br>• Rigorous testing and validation were carried out to assess the performance of the implemented algorithms. By evaluating various metrics and conducting thorough analyses, I identified areas for optimization and fine-tuning. This iterative process helped me enhance the performance of the models and ensure their accuracy and reliability.

                        <br>• Collaboration with cross-functional teams was an integral part of my project work. I leveraged my technical expertise and actively collaborated with team members from diverse backgrounds. By fostering effective teamwork, I contributed to the successful completion of challenging projects and delivered high-quality outcomes.

                        <br>• Demonstrating a strong grasp of statistical concepts and machine learning algorithms, I applied them to solve complex problems. This involved feature engineering techniques to enhance model performance, evaluating and selecting appropriate evaluation metrics, and optimizing hyperparameters to achieve optimal results.

                        <br>• Actively participating in discussions on data-driven decision-making, I provided valuable insights based on thorough analysis. By drawing on my expertise in data analysis and machine learning, I contributed to strategic discussions and supported evidence-based decision-making processes.

                        <br>• Throughout the program, I acquired practical experience in various areas, including feature engineering, model evaluation, and hyperparameter optimization techniques. These hands-on experiences equipped me with valuable skills that are applicable to real-world data science projects.

                        <br>• Displaying a commitment to continuous learning, I stayed up-to-date with the latest advancements in machine learning and data science. I actively sought out new knowledge, explored emerging techniques, and applied them to enhance my project work, ensuring that I remain at the forefront of the field.

                        <br>Overall, the Cognizant Virtual Experience Program provided me with a rich and diverse experience in data analysis and machine learning. Through analyzing complex datasets, developing predictive models, and effectively communicating findings, I am well-prepared to contribute to a dynamic team in the field of data science and drive impactful outcomes.
            </section>


            <section id="fourth">
                <header>
                

                    <h3>British Airways Virtual Data Science Experience Program – June 2023:</h3>          
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://forage-uploads-prod.s3.amazonaws.com/completion-certificates/British%20Airways/NjynCWzGSaWXQCxSX_British%20Airways_FH77MGBRcCReLyPrn_1686053559094_completion_certificate.pdf" class="button primary large">Certificate link</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-06-10 180342.jpg" alt=""></span>
                </header>
                <div class="content">
                    During my participation in the Virtual Experience Program in Data Science for British Airways in partnership with Forage, I had the opportunity to work on challenging projects and apply my skills in various areas of data science. Here are the key highlights of my work:

                    <br>  • During my participation in the Virtual Experience Program in Data Science for British Airways in June 2023, I had the opportunity to gain valuable hands-on experience in various areas of data science. Throughout the program, I focused on web scraping, machine learning, data visualization, and predicting customer buying behavior, enhancing my skills in these domains. Additionally, I acquired practical experience in feature engineering, model evaluation, and hyperparameter optimization techniques, allowing me to develop robust and efficient models.

                    <br>• One notable aspect of my work during the program involved leveraging natural language processing (NLP) techniques to extract and analyze customer review data. By applying NLP sentiment analysis techniques, I was able to uncover valuable insights regarding customer sentiment and preferences. This allowed me to understand the underlying emotions and opinions expressed in the customer reviews, enabling me to make informed data-driven decisions.
                    
                    <br>  •In order to optimize customer satisfaction and loyalty, I developed predictive models using various machine learning algorithms. These models were designed to forecast future trends and make data-driven recommendations. By leveraging historical customer data and incorporating features derived from web scraping, I was able to accurately predict customer buying behavior and provide personalized recommendations. This approach aimed to enhance customer experiences and foster long-term loyalty.
                    
                    <br>  •Throughout the project, I emphasized the importance of effective data visualization techniques. By employing advanced visualization libraries and tools, I was able to create visually appealing and informative representations of complex data findings. This enabled me to effectively communicate the insights and patterns I discovered, making the information more accessible and engaging for stakeholders. The integration of NLP sentiment analysis results into the data visualizations further enhanced the depth and richness of the presented information.
                    
                    <br>  •Moreover, I demonstrated a strong dedication to ongoing learning throughout the program. I actively sought to stay abreast of the latest advancements and breakthroughs in the fields of machine learning and data science. By staying updated with the most recent developments, methodologies, and best practices, I ensured that my work remained current and aligned with industry standards.
                    
                    <br>  •Overall, my participation in the Virtual Experience Program in Data Science for British Airways in June 2023 provided me with a comprehensive understanding of web scraping, machine learning, data visualization, and predictive modeling. The incorporation of NLP sentiment analysis techniques into my work allowed for deeper insights into customer preferences. I am excited to apply these skills and knowledge to real-world challenges, contribute to a dynamic team, and drive data-driven decision-making in the field of machine learning and data science.
                    
                    
            </section>

            <!-- Section -->
                <section id="fifth">
                    <header>
                        <h2>MSc Project:</h2>
                        <h3>Tuberculosis, Pneumonia, and Healthy and Infected lung
                            detection using Convolutional Neural Networks: VGG16,
                            VGG19, and ResNet50.</h3>
                        <ul class="actions">
                            <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                            <br><li><a href="https://github.com/AbhiSaste5402/MSc-Project-Part1-Pneumonia-detection-using-tansfer-learning-on-ResNet50-VGG16-and-VGG19" class="button primary large">Part 1</a></li></ul>
                            <ul class="actions"><br><li><a href="https://github.com/AbhiSaste5402/MSc-Project-Part2-Tuberculosis-detection-using-VGG16-VGG19-and-ResNet50" class="button primary large">Part 2</a></li></ul>
                                <ul class="actions"><br> <li><a href="https://github.com/AbhiSaste5402/MSc-Project--Part3-Classification-of-healthy-Tuberculosis-and-Pneumonia-chest-x-ray" class="button primary large">Part 3</a></li></ul>
                                    <ul class="actions"><br> <li><a href="https://github.com/AbhiSaste5402/MSc-Project-Part4-Normal-and-infected-lungs-detection-using-VGG16-VGG19-and-ResNet50" class="button primary large">Part 4</a></li></ul>
                                        <ul class="actions"><br> <li><a href="https://github.com/AbhiSaste5402/MSc-Project-Prediction-for-all-the-parts" class="button primary large">Results for all Parts</a></li></ul>

                        
                        
                        
                        <span class="image main"><img src="Screenshot 2023-05-08 162811.jpg" alt=""></span>

                    </header>

                    <div class="content">
                        <p><strong>Project Description:</strong><br>

                        The main objective of this project was to detect tuberculosis, pneumonia, and healthy and infected lungs from X-ray images using convolutional neural networks (CNNs). Three popular CNN architectures, VGG16, VGG19, and ResNet50, were used for the detection task. <br>Transfer learning was used for pneumonia detection using the VGG and ResNet models, which allowed for faster and more accurate training by utilizing pre-trained weights from a larger dataset. For tuberculosis detection, the models were trained from scratch without transfer learning.

                            The best model was then used to classify X-rays into healthy, pneumonia, or tuberculosis categories. The project also included executing predictions on individual models for their designated functions.
                            
                            <br>To evaluate the performance of the models, the variability in the training and testing accuracies of all the mentioned models was analyzed on different train-test split ratios trained on different numbers of epochs. This analysis helped to identify the optimal training configuration for each model, and provided insights into the robustness of the models to variations in the training and testing data.
                            
                            <br>Skills and tools used in this project included data visualization, data preparation, model building and evaluation, OpenCV, Python, NumPy, Pandas, TensorFlow, and Scikit-learn. These tools were used to clean and preprocess the data, train and evaluate the models, and visualize the results.
                            
                            <br>Overall, this project demonstrated the potential of CNNs for accurately detecting lung infections and diseases from X-ray images, and provided insights into the performance of different CNN architectures for this task. The results of this project could be useful in developing more accurate and efficient diagnostic tools for lung diseases.
                    </div>
                </section>

                <!-- Section -->
            <section id="sixth">
                <header>
                    <h2>Intership Projects:</h2>

                    <h3>1. Flight Fare Predictions</h3>          
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/Flight-Fare-Predictions" class="button primary large">GitHub</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-05-08 010703.jpg" alt=""></span>

                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>
                        This project aimed to develop a machine learning model that could accurately predict airfare for flights, a critical aspect of travel expenses. The project involved collecting and pre-processing data on flight routes, airline carriers, and historical ticket prices to train and test various machine learning models.

<br><br>The performance of different machine learning algorithms, including linear regression, decision trees, and neural networks, were evaluated to determine the most accurate and efficient model for predicting flight fares. The best-performing model was then integrated into a web-based application that provided real-time airfare predictions.

<br><br>Python was used as the primary programming language for implementing the machine learning algorithms and developing the web application. Data cleaning and pre-processing were performed using Python libraries such as Numpy and Pandas to transform the raw data into a format suitable for machine learning algorithms.

<br><br>Matplotlib was utilized for data visualization to create various plots and charts that helped visualize the data and performance of the machine learning models. Sklearn, a popular Python library for machine learning tasks, was used to train and test the machine learning models. Jupyter notebook and Visual Studio Code were used as the integrated development environments (IDEs) to write and execute Python code and visualize the data.

<br><br>Python Flask, a web framework, was used to create the HTTP server for the web application, handle user requests, and provide responses. HTML/CSS were used to create the user interface for the web application.

<br><br>Amazon Elastic Compute Cloud (EC2), a cloud computing service, was utilized to deploy the web application and make it available to users over the internet. The EC2 instance was configured to run the web server and host the web application. Variability in the training and testing accuracies of the different machine learning models trained on different numbers of epochs and using various train-test split ratios was also analyzed.</p>
</div>
            </section>

            <!-- Section -->
            <section id="seventh">
                <header>

                    <h3>2. Campus Placement</h3>
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/Campus-placement" class="button primary large">GitHub</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-05-08 011342.jpg" alt=""></span>

                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>
                       
This project involved the development of machine learning models trained on a dataset with various selection criteria, including academic scores and work experience. The grid search cross-validation method was used to determine the highest accuracy model, which was then utilized for prediction and the user interface. The model's accuracy was increased by tuning its parameters before being deployed on a cloud platform.

<br><br>The project yielded a testing accuracy of 79.06% and a training accuracy of 91.27%, demonstrating the potential of machine learning to improve recruitment processes and provide a useful tool for students to know their chances of getting placed. The project addressed the challenges faced by recruiters and students during the placement process, benefitting both parties.

<br><br>The project utilized various tools and skills, including Python for implementing the machine learning algorithms and developing the web application. Numpy and Pandas were used for data cleaning and preprocessing, while Matplotlib was used for data visualization. Sklearn provided the necessary algorithms and tools for machine learning, and Jupyter notebook and Visual Studio Code were utilized as integrated development environments. Flask was used for developing the HTTP server and handling user requests, and HTML/CSS were used for creating the user interface. Finally, an AWS EC2 instance was utilized to deploy the web application and make it available to users over the internet.
                    </p>
                </div>
            </section>

            <section id="eighth">
                <header>
                    <h2>Personal Projects</h2>

                    <h3>Sentiment Analysis Model for Stocks with News Headlines using NLP</h3>
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/Sentiment-Analysis-Model/blob/main/Sentiment%20Analysis%20Model%20for%20Stocks%20using%20News%20Headlines.ipynb" class="button primary large">GitHub</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-05-09 224716.jpg" alt=""></span>
                    <span class="image main"><img src="Screenshot 2023-05-09 224638.jpg" alt=""></span>

                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>

                    
                        The goal of the project is to analyze the relationship between world news headlines and stock price shifts using Natural Language Processing (NLP) techniques. The dataset used contains 25 columns of top news headlines for each day, spanning from 2008 to 2016. The labels are based on the Dow Jones Industrial Average stock index, where class 1 indicates an increase in stock price and class 0 indicates no change or a decrease in stock price.

                        <br>The project begins by importing the necessary libraries, including Pandas for data manipulation, Matplotlib and Seaborn for data visualization, and Scikit-learn for machine learning. The dataset is loaded into a Pandas DataFrame and the columns are inspected. The data is then visualized to gain insights into the data distribution and identify any trends.
                        
                        <br> The next step is to preprocess the data for NLP. The data is cleaned by removing punctuations and converting all text to lowercase to ensure consistency. A CountVectorizer is used to extract features from the text by converting the text into a matrix of token counts. The data is then split into training and testing datasets to train the machine learning model.
                        
                        <br> The RandomForestClassifier is used as a machine learning model for training. The model is trained on the training dataset, and its performance is evaluated on the testing dataset. The accuracy score is used to measure the performance of the model, which achieved an accuracy of 84.12% over the test data.
                        
                        <br>The results of the project show that there is a relationship between news headlines and stock price shifts. The machine learning model is able to predict the stock price changes based on the news headlines, which has implications for investors and financial analysts. The project can inform investment decisions and aid in risk management.
                        
                        <br>In conclusion, this project utilized NLP techniques to analyze the relationship between world news headlines and stock price shifts. The project highlights the importance of data preprocessing and feature extraction for NLP tasks. The machine learning model's performance shows that there is a relationship between news headlines and stock price shifts, which has practical implications for investors and financial analysts.
                    </p>
                </div>
            </section>


            <section id="ninth">
                <header>
                    <h3>Customer Segmentation Model</h3>
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/Customer-Segmentation" class="button primary large">GitHub</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-05-13 185517.jpg" alt=""></span>
                    <span class="image main"><img src="Screenshot 2023-05-13 185533.jpg" alt=""></span>

                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>
                        The objective of the project was to assist an automobile company in entering new markets with their existing products and identifying the right group of new customers based on their behavior. <br>This was achieved by employing the process of clustering to group similar observations together based on their features.

The first step in the process involved preparing the data for clustering and gaining insights through data visualization. <br>Subsequently, three methods were used to determine the optimal value for k, which is the number of clusters. The silhouette, WCSS, and Calinski-Harabasz methods were employed to evaluate the quality and effectiveness of clustering by measuring the differences between clusters and their compactness.<br>

After determining the optimal value for k, the data was trained on k-means clustering, which is a popular method of clustering that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. <br>K-means clustering is a relatively simple and efficient algorithm that works well for large datasets with a large number of observations.

<br>The results of the clustering can enable the automobile company to perform segmented outreach and communication for different segments of customers, tailoring marketing strategies and communication to specific customer segments. <br>Clustering is a powerful technique for segmenting customers and identifying patterns in large datasets with various applications in marketing, finance, and healthcare. For instance, in marketing, clustering can be used to segment customers based on demographics, behavior, or other factors to personalize marketing campaigns and increase customer retention.
                    </p>
                </div>
            </section>

            <section id="tenth">
                <header>
                    <h3>Fraud Detection Models</h3>
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/Fraud-Detection-Model-Credit-card-Anamoly-Detection-/tree/main" class="button primary large">GitHub</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-05-13 193803.jpg" alt=""></span>
                    <span class="image main"><img src="Screenshot 2023-05-13 193817.jpg" alt=""></span>
                    <span class="image main"><img src="Screenshot 2023-05-13 193841.jpg" alt=""></span>
                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>
                        This project involves the detection of credit card fraud using anomaly detection techniques. The dataset used in this project contains credit card transactions made by European cardholders in September 2013, and comprises 284,807 transactions, out of which only 492 are fraudulent, resulting in a highly imbalanced dataset where the positive class (frauds) account for 0.172% of all transactions.

                        <br>The input features of the dataset are numerical and have been obtained through a Principal Component Analysis (PCA) transformation. The transformed features are labeled as V1, V2, ..., V28, while the remaining features that have not undergone PCA transformation are Time and Amount. The Time feature captures the time elapsed in seconds between each transaction and the first transaction in the dataset, while the Amount feature represents the monetary amount involved in each transaction.
                        
                        <br>The primary objective of this project is to detect fraudulent transactions in the dataset using different anomaly detection techniques and comparing their performance. Three different techniques have been used for this purpose: Isolation Forest, Local Outlier Factor (LOF), and Support Vector Machines (SVM). 
                        
                        <br>The results of the project show that the Isolation Forest algorithm outperforms the other two algorithms in terms of accuracy, precision, and recall. Specifically, the Isolation Forest algorithm detected 73 errors, which is less than the 97 errors detected by the LOF algorithm and significantly less than the 8516 errors detected by the SVM algorithm. Furthermore, the Isolation Forest algorithm achieved an accuracy rate of 99.74%, which is higher than the LOF accuracy rate of 99.65% and much higher than the SVM accuracy rate of 70.09%. 
                        
                        <br>When evaluating the detection rate of fraud cases, the Isolation Forest algorithm performed significantly better than the LOF algorithm, with a detection rate of 27% compared to LOF's detection rate of only 2%. The SVM algorithm was unable to detect any fraudulent transactions. 
                        
                        <br>Overall, this project demonstrates the effectiveness of the Isolation Forest algorithm for detecting fraudulent credit card transactions. The results suggest that increasing the sample size or using deep learning algorithms could improve the accuracy further, but at the cost of increased computational expense. Furthermore, the use of complex anomaly detection models could potentially enhance the accuracy of fraud detection even further.
                    </p>
                   

                </div>
            </section>

            <!-- Section -->
            <section id="eleventh">
                <header>
                    <h3>Building a Deep Neural Network for predicting the motion detection from the data collected by the IoT sensors.</h3>
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/predicting-the-motion-detection-from-the-data-collected-by-the-IoT-sensors" class="button primary large">GitHub</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-05-08 011534.jpg" alt=""></span>

                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>
                        The main objective of this project was to create a deep learning model, specifically a CNN, that could accurately detect room occupancy based on environmental sensor data. The sensor data used included temperature, humidity, and Co2, which were collected by an IoT system consisting of three sensors placed in different locations with varying environmental conditions.

The data collected was stored in a file and used to train the CNN model to predict the "motion" feature, which indicates the presence of movement in proximity to the sensors. Keras, a popular deep learning framework, was used to implement and optimize the model's hyperparameters.

Once the model was developed and optimized, its performance was evaluated by analyzing various metrics such as accuracy, precision, and recall. The results were presented in a technical report, which included diagrams, plots, tables, and numerical data to help visualize and interpret the model's performance.

Overall, this project demonstrated the potential of using CNN models and environmental sensor data to detect room occupancy, which could have practical applications in areas such as building automation, energy management, and security systems.</p>
                </div>
            </section>

            <!-- Section Online_Learning.jpg -->
            <section id="tweleveth">
                <header>
                    <h3>House Price Prediction</h3>
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/Banglore-House-Prediction-app" class="button primary large">GitHub</a></li>
                    </ul>
                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>
                        This project aimed to develop a machine learning model to accurately predict the selling price of houses. The accurate predictions would help homeowners and real estate agents determine the optimal selling price for their properties, while also helping buyers make informed decisions when purchasing a home.

<br><br>The project involved collecting and processing data on various features that influence house prices, such as the number of bedrooms, bathrooms, location, square footage, and age of the house. The collected data was pre-processed to remove any inconsistencies or outliers and then used to train and test various machine learning models.

<br><br>This data was then trained on three different models, namely Linear Regression, Lasso, and Decision Tree Regressor to find out the highest accuracy from all using the Grid Search method. Among these models, the Linear Regression model was found to be the most accurate, with a total accuracy of 81.83%. This model was used for both the training and the predictions.

<br><br>Tools and skills used in this project included Python, Numpy, Pandas, Matplotlib, Sklearn, Jupyter notebook, Visual Studio Code, and AWS EC2 instance. Python was used to implement the machine learning algorithms and develop the web application, while Numpy and Pandas were used for data manipulation and analysis. Matplotlib was used for data visualization, and Sklearn provided various algorithms and tools for classification, regression, clustering, and more. Jupyter notebook and Visual Studio Code were used to write and execute Python code, visualize the data, and develop the web application. Flask, HTML, and CSS were used to create the HTTP server, user interface, and styling of the web application. Finally, AWS EC2 instance was used to deploy the web application and make it available to users over the internet.</p>
                    <!-- <span class="image main"><img src="images/OnlineLearning.jpg" alt=""></span> -->
                </div>
            </section>

            <!-- Section ALL -->


            
            <section id="thirteenth">
                <header>
                    <h3>Car Brand Classification</h3>
                    <ul class="actions">
                        <!---<li><a href="https://github.com/Hammad-Mir/WSSS4LUAD" class="button large">GitHub</a></li>--->
                        <li><a href="https://github.com/AbhiSaste5402/Car-Classification" class="button primary large">GitHub</a></li>
                    </ul>
                    <span class="image main"><img src="Screenshot 2023-05-08 005442.jpg" alt=""></span>
                    <span class="image main"><img src="Screenshot 2023-05-08 010038.jpg" alt=""></span>

                </header>
                <div class="content">
                    <p><strong>Project Description:</strong><br>
                        The goal of this project was to develop a deep learning model that could accurately classify car brands based on custom data. The model was built using a ResNet architecture and transfer learning techniques to leverage pre-trained models that have been trained on large datasets. The pre-trained model was fine-tuned on the custom dataset to improve its accuracy and adapt it to the task at hand.

                        The dataset consisted of images of cars from different brands, which were preprocessed to ensure consistency and quality. The images were split into training, validation, and testing sets to evaluate the performance of the model.
                        
                        The ResNet model was implemented using Keras and optimized using techniques such as learning rate scheduling and early stopping to achieve the best possible results. The performance of the model was evaluated using various metrics, such as accuracy, precision, and recall, and the results were presented in a technical report.
                        
                        To further improve the model's performance, data augmentation techniques were used to increase the diversity and size of the training dataset. This helped the model generalize better to new, unseen images.
                        
                        Overall, this project demonstrated the effectiveness of using ResNet and transfer learning techniques to develop a deep learning model for car brand classification on custom data. The model could have practical applications in areas such as automotive manufacturing, advertising, and marketing.
                    </p>
                </div>
            </section>

            
            <!-- Section -->
                <section>
                    <header>
                        <h2>Get in touch</h2>
                    </header>
                    <div class="content">
                        <ul class="items">
                            <li>
                                <p><Strong>Email:</Strong>
                                <a href="#">abhisaste5402@gmail.com</a></p>
                            </li>
                            <li>
                                <p><Strong>Phone Number:</Strong> <a href="#">+44 7471327623</a></p>
                               
                            </li>
                            <li>
                                <p><Strong>Address:</Strong> London, United Kingdom
                                </p>
                            </li>
                            <li>
                                <p><Strong>Social:</Strong> <ul class="icons">
                                    <li><a href="https://www.linkedin.com/in/abhishek-saste-3260501b5/" class="button primary large">LinkedIn Profile</a></li>
                                    <li><a href="https://github.com/AbhiSaste5402?tab=repositories" class="button primary large">GitHub Profile</a></li>

                                </ul>
               </p>
                                            </li>
                        </ul>
                    </div>
                    
                  
                    -->
                </section>

            <!-- Copyright -->
                <div class="copyright">© Untitled. All rights reserved. Design: <a href="https://html5up.net">HTML5 UP</a>.</div>

        </div>

    <!-- Scripts -->
        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.scrolly.min.js"></script>
        <script src="js/browser.min.js"></script>
        <script src="js/breakpoints.min.js"></script>
        <script src="js/util.js"></script> 
        <script src="C:\Users\Abhishek\Desktop\porti\js\main.js"></script>  


</section></div></body></html>
